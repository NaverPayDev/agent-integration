#!/usr/bin/env python3
# Npay agent-payments-integration
# Copyright (c) 2025-present NAVER FINANCIAL Corp.
# Apache-2.0

"""
ë¬¸ì„œì—ì„œ í•œêµ­ì–´ ëª…ì‚¬(ë³µí•©ëª…ì‚¬, ë‹¨ì¼ëª…ì‚¬)ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ì¸ë±ì‹±ìš© ì‚¬ì „ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

import json
import os
import re
import sys
import glob
import argparse

from PyKomoran import Komoran


def create_noun_dict(document, komoran):
    """ë¬¸ì„œì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ì‚¬ì „ êµ¬ì¶•"""
    noun_dict = {}

    tokens = re.split(r'[\s\n\t.,;:!?\'"()\[\]{}~`@#$%^&*\-_+=|\\<>/]+', document)
    tokens = list(filter(None, tokens))
    total = len(tokens)

    for idx, token in enumerate(tokens, 1):
        percent = (idx / total) * 100
        print(f"\rğŸ” ëª…ì‚¬ ì¶”ì¶œ ì¤‘... {percent:.1f}% ({idx}/{total})", end="")
        sys.stdout.flush()

        tagged = komoran.get_plain_text(token)
        tagged_tokens = tagged.strip().split()

        noun_morphs = []
        for t in tagged_tokens:
            morph, tag = t.rsplit("/", 1)
            if tag in ["NNG", "NNP", "NNB", "NP", "NR"]:
                noun_morphs.append(morph)
            else:
                break

        if noun_morphs:
            noun_word = "".join(noun_morphs)
            if len(noun_word) >= 2:
                if len(noun_morphs) == 1:
                    noun_dict[noun_word] = [noun_word]
                else:
                    noun_dict[noun_word] = noun_morphs + [noun_word]

    print()
    return noun_dict


def build_unified_noun_dict(file_paths):
    """ê° íŒŒì¼ë³„ë¡œ ëª…ì‚¬ ì‚¬ì „ êµ¬ì¶• í›„ í†µí•©"""
    komoran = Komoran("EXP")
    unified_noun_dict = {}

    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                document = f.read()

            noun_dict = create_noun_dict(document, komoran)
            unified_noun_dict.update(noun_dict)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {file_path} - {e}")

    return unified_noun_dict


def main():
    parser = argparse.ArgumentParser(
        description="ë¬¸ì„œì—ì„œ í•œêµ­ì–´ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ì¸ë±ì‹±ìš© ì‚¬ì „ êµ¬ì¶•"
    )
    parser.add_argument(
        "files", nargs="+", help="ëª…ì‚¬ë¥¼ ì¶”ì¶œí•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ (ì™€ì¼ë“œì¹´ë“œ ì§€ì›)"
    )
    parser.add_argument(
        "-o",
        "--output",
        default="noun_dictionary.json",
        help="ëª…ì‚¬ ì‚¬ì „ JSON íŒŒì¼ëª…",
    )

    args = parser.parse_args()

    file_paths = []
    for pattern in args.files:
        file_paths.extend(glob.glob(pattern))

    file_paths = list(set(file_paths))

    if not file_paths:
        print("âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        sys.exit(1)

    noun_dict = build_unified_noun_dict(file_paths)
    sorted_dict = dict(sorted(noun_dict.items(), key=lambda x: len(x[0]), reverse=True))

    os.makedirs("out", exist_ok=True)
    output_path = os.path.join("out", args.output)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(sorted_dict, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“™ ëª…ì‚¬ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {output_path}")


if __name__ == "__main__":
    main()
